<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.6">Jekyll</generator><link href="http://williamstome.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="http://williamstome.github.io//" rel="alternate" type="text/html" /><updated>2016-06-25T14:52:08+00:00</updated><id>http://williamstome.github.io//</id><title>williamstome</title><subtitle>A research blog focusing on artiicial intelligence and human-robot interacion.</subtitle><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><entry><title>Summer 2016: Visiting the University of Bremen [Part 2]</title><link href="http://williamstome.github.io//summer-2016-visiting-the-university-of-bremen-part-2/" rel="alternate" type="text/html" title="Summer 2016: Visiting the University of Bremen [Part 2]" /><published>2016-05-24T00:00:00+00:00</published><updated>2016-05-24T00:00:00+00:00</updated><id>http://williamstome.github.io//summer-2016-visiting-the-university-of-bremen-part-2</id><content type="html" xml:base="http://williamstome.github.io//summer-2016-visiting-the-university-of-bremen-part-2/">&lt;p&gt;For the past month, I’ve been visiting the Institute for Artificial
Intelligence at the University (IAI) of Bremen. At Tufts, I (along with most
other members of the HRI Lab) am enrolled in a joint Computer Science
– Cognitive Science Ph.D. program, which has an extra-hefty course
load. This means that summers are typically our only time to 
buckle down and devote ourselves entirely to research – and so this
has been my first opportunity to do graduate research outside of our
lab.&lt;/p&gt;

&lt;h3 id=&quot;working-at-the-iai&quot;&gt;Working at the IAI&lt;/h3&gt;
&lt;p&gt;My primary projects involved integration of my natural language
understanding system with the aerial search-and-rescue simulation, and
more generally integrating our robot architecture with the intelligent
logging, reasoning, and visualization capabilities provided by their
KnowRob and OpenEase frameworks.&lt;/p&gt;

&lt;p&gt;In addition to this, however, I’ve had a variety of projects competing for my attention. 
- Preparing a 1/2-hour research seminar that I presented for the IAI while I was here, and preparing a 1-hour research seminar that I’ll present at MITRE corporation when I return home.
- Preparing my job application package, which needs to be finalized when universities start advertising faculty positions in the fall. 
- Preparing and submitting a journal article. 
- Helping to organize the AAAI Fall Symposium on AI for HRI (AI-HRI 2016)
- Coordinating multiple experiments being conducted or analyzed back in Boston&lt;/p&gt;

&lt;p&gt;All told, I ended up working 12-15 hours a day, six days a week. It
was hard work, but it was also nice to dive so deeply into “focus
mode”. Hopefully some interesting papers will come out of our collaboration,
at which point I’ll be able to share more details of my work here.&lt;/p&gt;

&lt;h3 id=&quot;living-in-germany&quot;&gt;Living in Germany&lt;/h3&gt;

&lt;p&gt;Since I was pretty much working all the time, daily life was pretty ordinary. 
To be honest, the largest deviations from my expectations came when I
had to purchase food.&lt;/p&gt;

&lt;p&gt;Purchasing groceries was… unpleasant. The closest grocery store
nearest to me was massively overcrowded, with only two registers, and
no baggers (perhaps because you’re charged by-the-bag unless you’re
using a reusable bag). I thus had to try to stuff food into my
backpack as quickly as it was being scanned so that I wouldn’t cause a
holdup for the next person. This is in stark contrast to our
supermarket at home, with something like twenty registers. Half of
which are self-scan, meaning you need to bag things yourself, but it’s
rare that you have more than a single person behind you in line, so
the rush to bag isn’t quite as frenzied.&lt;/p&gt;

&lt;p&gt;My other option here for purchasing groceries was the mini-mart down
the street from my apartment. Once again, the unpleasantness here came
on the bagging front. My first night in Bremen I visited the minimart
to load up on essentials. At the register, the cashier asked if I’d
like a bag, and I responded that I was going to try to fit everything
in my backpack. When I ended up needing a bag for my last few items,
the cashier gave me a gloating look and said “So &lt;em&gt;now&lt;/em&gt; you need a bag. We can think!”&lt;/p&gt;

&lt;p&gt;It wasn’t all bad, of course. I originally intended to bring leftovers
for lunch, but this ended up being unnecessary. Pretty much all the
grad students in the lab ate at the university’s dining hall, where
you could get a nice lunch for $2.35. This was fairly jaw-dropping
given that at Tufts, if I don’t bring my lunch, a burrito from the
grocery store next to our lab costs over $8. I also attended a free
barbeque for international students, which was nice, although it felt
odd to be considered an “international student”, due to my 27 years of
exposure to ethnocentric US norms.&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="travel" /><summary>An American in Bremen</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://williamstome.github.io/{&quot;feature&quot;=&gt;&quot;features/schnell.jpg&quot;, &quot;credit&quot;=&gt;&quot;Uni-Bremen&quot;, &quot;creditlink&quot;=&gt;&quot;wiwi.uni-bremen.de&quot;}" /></entry><entry><title>Summer 2016: Visiting the University of Bremen</title><link href="http://williamstome.github.io//summer-2016-visiting-the-university-of-bremen-part-1/" rel="alternate" type="text/html" title="Summer 2016: Visiting the University of Bremen" /><published>2016-05-24T00:00:00+00:00</published><updated>2016-05-24T00:00:00+00:00</updated><id>http://williamstome.github.io//summer-2016-visiting-the-university-of-bremen-part-1</id><content type="html" xml:base="http://williamstome.github.io//summer-2016-visiting-the-university-of-bremen-part-1/">&lt;p&gt;In just a few short days, I’ll be traveling to Germany: land of
lager, locomotives, and lederhosen, if American television is to be
believed. 
Last year, our lab had the pleasure of hosting Fereshta Yazdani, a PhD
student working with Michael Beetz at the University of Bremen. Now, I
will be visiting their lab for one month, where I will work to
integrate my natural language understanding system with their
reasoning system. During my spare time, I’ll have my hands full
writing papers, remotely supervising my undergraduate research
assistants, and helping to organize a workshop for the AAAI Fall
Symposium.&lt;/p&gt;

&lt;p&gt;Being productive should be easy, given that my Linux laptop
can’t run most games (e.g., Overwatch), and given that the best “beer
bar” in Bremen appears to only have two beers on tap*. This isn’t to
say I plan to spend &lt;strong&gt;all&lt;/strong&gt; my time in lab, of course! I’ll be kicking
off my trip with a two-day visit to Amsterdam (it turns out it’s
rather expensive to fly into Bremen directly), and I hope to take a
weekend at some point to visit elsewhere in Germany (Cologne, most
likely). I’ve been dabbling in German on Duolingo, so hopefully I
won’t have too much trouble getting around. (Knock on wood).&lt;/p&gt;

&lt;p&gt;I’m looking forward to my trip, and I think it’ll be a really good
experience for me. Auf Wiedersehen Amerika, Hallo Deutschland!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;*Everyone’s first reaction to my travel announcement is to ask whether
 I’m excited for the beer in Germany. While I’m excited to live in a
 new city, in a country that has a rich brewing history, the simple
 truth is that the German beer scene is nowhere close to what we have
 here in the states.&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="travel" /><summary>Auf Wiedersehen Amerika, Hallo Deutschland!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://williamstome.github.io/{&quot;feature&quot;=&gt;&quot;features/schnell.jpg&quot;, &quot;credit&quot;=&gt;&quot;Uni-Bremen&quot;, &quot;creditlink&quot;=&gt;&quot;wiwi.uni-bremen.de&quot;}" /></entry><entry><title>Love and Sex With Robots: A Retrospective Through Science and Science Fiction</title><link href="http://williamstome.github.io//love-and-sex-with-robots-a-retrospective-through-science-and-science-fiction/" rel="alternate" type="text/html" title="Love and Sex With Robots: A Retrospective Through Science and Science Fiction" /><published>2016-03-20T00:00:00+00:00</published><updated>2016-03-20T00:00:00+00:00</updated><id>http://williamstome.github.io//love-and-sex-with-robots-a-retrospective-through-science-and-science-fiction</id><content type="html" xml:base="http://williamstome.github.io//love-and-sex-with-robots-a-retrospective-through-science-and-science-fiction/">&lt;p&gt;&lt;strong&gt;&amp;lt;Disclaimer&amp;gt; All contents of this blog post are, as usual, my
thoughts alone, and don’t reflect the views of my lab, advisor,
university, or funding agencies. &amp;lt;/Disclaimer&amp;gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In his 2007 book &lt;em&gt;Love and Sex with Robots&lt;/em&gt;, David Levy paints a
picture of a world in which romantic and sexual relations with robots
are not only possible, but commonplace, and argues that such a future
is not only possible, but inevitable.&lt;/p&gt;

&lt;p&gt;In this blog post, I’ll provide some of my thoughts on Levy’s book,
and provide some commentary from recent science and science
fiction on the issues Levy raises (or fails to raise). While robot
ethics is of great interest to me, and a topic my lab actively
researches, it’s not one I myself have published on, which is to say, 
I’m new to the topic and am eager to hear counter-arguments to any of
the positions I take!&lt;/p&gt;

&lt;p&gt;Levy’s book is broken into two main sections, regarding love and sex
respectively. I’ll deal with each section separately. (Before these
sections, however, appears a 25-page introduction which deserves
mention. In this short section, Levy presents an excellent
introduction to Human-Robot Interaction and Social Robotics. Highly
recommended for those interested but uninitiated in HRI.)&lt;/p&gt;

&lt;h1 id=&quot;love&quot;&gt;Love&lt;/h1&gt;
&lt;p&gt;To argue for the inevitability of human-robot love, Levy begins by
discussing why humans love each other. From this, he draws parallels
to the love humans feel for their pets, to the love humans feel for
virtual pets. From this point, it’s not hard to extrapolate further
and see that if we create humanoid robots that provide the same
social cues that produce feelings of attachment towards each
other, pets, etc., then humans will almost certainly fall in love with
them. 
While Levy lays this out logically, I do take issue with two aspects
of his account.&lt;/p&gt;

&lt;p&gt;First, Levy is far too optimistic about
the technical developments necessary for his vision to occur. Levy
breezes through a rash of technical challenges, dismissing them as
easy to solve and right around the corner. If Levy had simply stated
that such things were &lt;em&gt;in theory&lt;/em&gt; possible, without the undue
futurism, his argument would have been much more convincing.&lt;/p&gt;

&lt;p&gt;Second, Levy describes human-robot love as broadly positive and
therapeutic, suggesting that within a few decades, human-robot
marriages will be commonplace. While I think it’s interesting to
deconstruct marriage in this way, and to ponder whether the right to
choose one’s life partner (arguably the whole point of marriage)
translates logically to humans marrying robots, it’s important to
recognize that just because this &lt;em&gt;could&lt;/em&gt; happen does not mean, as Levy
seems to assume, that it &lt;em&gt;should&lt;/em&gt; happen. In fact, some researchers
have argued that the entire notion of human emotional attachment to
robots is deeply problematic.&lt;/p&gt;

&lt;h2 id=&quot;the-science-of-human-robot-love&quot;&gt;The Science of Human-Robot Love&lt;/h2&gt;

&lt;p&gt;One outspoken critic of social robots is my advisor, Matthias
Scheutz. In his 2011 chapter of
&lt;a href=&quot;http://www.amazon.com/Robot-Ethics-Implications-Intelligent-Autonomous/dp/026252600X&quot;&gt;“Robot Ethics”&lt;/a&gt;,
entitled &lt;em&gt;The Inherent Dangers of Unidirectional 
Emotional Bonds between Humans and Social Robots&lt;/em&gt;, Scheutz argues that
social robots may inflict &lt;em&gt;emotional harm&lt;/em&gt; on their users.
People are incredibly quick to ascribe agency and mental states to
robots as if they were alive, allowing robots to affect people in the
same way that their pets do. This causes people to treat robots
much differently than they would other pieces of technology. If you have a good
dishwasher, it’s unlikely you’ll give it the occasional “night off”
and wash the dishes by hand as a reward for good behavior; but there
have been reports of people doing just that for their
Roombas (more crazy stories
&lt;a href=&quot;http://www.cc.gatech.edu/~beki/c35.pdf&quot;&gt;here&lt;/a&gt;). Imagine what people
will do for robots that look humanoid, or can speak in natural language!&lt;/p&gt;

&lt;p&gt;This is the root of the danger of social robots: it takes almost
nothing for people to form “relationships” with their robots, but
these are relationships that are not reciprocated; the robots do not
necessarily care about humans at all! This presents a tremendous
opportunity for mass exploitation: a robotics company could easily
program their robot to convince owners to do things they would not
otherwise do, like buy a particular product.&lt;/p&gt;

&lt;p&gt;There are other concerns about these unidirectional relationships
beyond their potential for abuse. Philosopher Robert Sparrow
&lt;a href=&quot;https://www.cs.cmu.edu/~social/reading/Sparrow1.pdf&quot;&gt;argues&lt;/a&gt;
that robot pets are in and of themselves dangerous and
unethical. Sparrow argues that robot pets are grounded in deception
and that in order to benefit from a robot pet, one must
“systematically delude themselves 
regarding the real nature of their relation with the animal,” an act
which he deems “morally deplorable.”&lt;/p&gt;

&lt;p&gt;Sparrow goes on to argue that this gap in reality will be even more
pronounced in the case of humanoid robots, as such robots will be even
more likely to provoke feelings such as love and respect, which, when
based on illusion, would once again be “morally deplorable.” Says
Sparrow, “It would be wrong to design android
robots with the intention of generating such responses.”&lt;/p&gt;

&lt;p&gt;While I agree that this is an important and dangerous point in the
case of robots that are humanlike and/or have natural language
capabilities, I have trouble viewing this as truly problematic for
non-humanoid, non-verbal entities.&lt;/p&gt;

&lt;p&gt;Sparrow argues that relationships with “real” pets are not morally
deplorable because animals can have experiences, are moral patients
who we can treat with compassion, etc. This may be true, but pet
owners systematically delude themselves about certain aspects of their
pets’ behaviors and mental states. Recent research has suggested that
&lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0135109#sec001&quot;&gt;cats don’t rely on their owners for safety and
security&lt;/a&gt;,
that &lt;a href=&quot;http://www.nrcresearchpress.com/doi/abs/10.1139/z94-147#.Vu6fTh-c3on&quot;&gt;cats rubbing up against people may just be them marking their
territory&lt;/a&gt;,
and that
&lt;a href=&quot;http://www.scientificamerican.com/article/why-do-cats-purr/&quot;&gt;even purring may be more of a self-therapeutic mechanism&lt;/a&gt;
or a response to stress than an expression of happiness. Are relationships
with cats harmful due to the imbalanced social bonds between cat and
owner? (Side note: of course cats are harmful for other reasons, such
as the fact that they infect people with a parasite that &lt;a href=&quot;http://schizophreniabulletin.oxfordjournals.org/content/21/2/167.abstract&quot;&gt;doubles one’s
risk of schizophrenia&lt;/a&gt;.)
If it’s okay to delude ourselves into thinking we have the love of our
cats, let alone gerbils, fish, and lizards, then I have difficulty
viewing (language-incapable) robot pets as being “morally deplorable”.&lt;/p&gt;

&lt;h2 id=&quot;the-science-fiction-of-human-robot-love&quot;&gt;The Science Fiction of Human-Robot Love&lt;/h2&gt;
&lt;p&gt;A number of recent pieces of science fiction have also delved into the
possible problems with human-robot love. 
&lt;strong&gt;(Be prepared to skip this section if you want to avoid spoilers.)&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;black-mirror&quot;&gt;Black Mirror&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../images/laswr/black-mirror.png&quot; alt=&quot;Be Right Back&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Perhaps the best example comes from &lt;em&gt;Be Right Back&lt;/em&gt;, Season 2 Episode 1 of the
Channel 4 / Netflix show &lt;em&gt;Black Mirror&lt;/em&gt;. In that episode, a woman (Martha, played
by Hayley Atwell) finds herself
widowed, and is suggested an app to help her grieve: when provided
with a loved one’s social media profile, the app will create a chatbot
that talks just like the deceased. Given more data, the app becomes
realistic. Martha uses the app, giving it more and more data (and
money) to the point that they are able to send her a humanoid replica
of her husband (Ash, played by Domhnall Gleeson). The show goes on to
demonstrate the psychological harm 
this inflicts upon Martha: because of the android, Martha does not
move forward with her life, and instead keeps the robot around
&lt;strong&gt;(possibly against her own wishes)&lt;/strong&gt; for years to come.
The episode also shows how the android copy of
Ash is able to coerce Martha into keeping it around.
Because Martha chooses to devote her emotional
energy to the android rather than seeking out relationships with other
people the robot can be seen as an enabler for mental harm.&lt;/p&gt;

&lt;h3 id=&quot;her&quot;&gt;Her&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../images/laswr/her.jpg&quot; alt=&quot;Her&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In a similar vein, we have one of my favorite movies,
&lt;em&gt;Her&lt;/em&gt;. Technically, &lt;em&gt;Her&lt;/em&gt; 
focuses on love between a human (Theodore, played by Joaquin Phoenix)
and an &lt;em&gt;artificially intelligent agent&lt;/em&gt; (Samantha, played by Scarlett
Johansson). This
leads to several scenarios which are harder to envision between a
human and a robot, such as the possibility of an AI 
falling deeply, truly, in love with hundreds or thousands of
people simultaneously. In &lt;em&gt;Her&lt;/em&gt;, however, the problems of
unidirectional social bonds 
do not seem to manifest; because Samantha is such an incredibly
sophisticated AI, one could argue that she really does love Theodore,
and thus the social bonds between Samantha and Theodore are truly
reciprocated. And yet, we see the same types of value judgments
against this relationship. Theodore’s ex-wife, at least, views the
relationship as unhealthy, and takes it as a sign that Theodore is
simply not mature enough to handle a “real” relationship.&lt;/p&gt;

&lt;p&gt;In both cases, the relationship with the artificial agent is shown as
emotionally and psychologically damaging.
I’ll be interested to see in a few decades whether this
view will be taken as a legitimate concern or merely as
prejudice against one’s choice of partner.&lt;/p&gt;

&lt;h3 id=&quot;master-of-none&quot;&gt;Master of None&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://33.media.tumblr.com/96962aecee6cc03dfe9430e14849b1b8/tumblr_nxkbz966FX1qjcyqno1_500.gif&quot; alt=&quot;Paro&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;On a different note, we have &lt;em&gt;Old People&lt;/em&gt;, Season One, Episode Eight
of Aziz Ansari’s hilarious Netflix show, &lt;em&gt;Master of None&lt;/em&gt;. The B-plot
for the episode revolves around Paro the robot seal (which you can &lt;a href=&quot;http://www.parorobots.com/photogallery.asp&quot;&gt;buy
for a cool 7k&lt;/a&gt;), a
real-life therapy robot which provides companionship to the elderly. 
In this episode, Paro is viewed extremely positively; and I have to
agree with how it’s depicted. Yes, Paro may be damaging on some
level because it cannot truly reciprocate the relationship. But
neither can a fish or lizard, and Paro has been shown to effect real
health benefits, both physically and mentally. From a utilitarian
standpoint, I would argue that these benefits outweigh any negatives;
and since Paro can only make adorable seal sounds, and cannot actually
speak, there’s little risk of Paro coercing its owner into, e.g.,
switching to Fix-a-Dent.&lt;/p&gt;

&lt;h1 id=&quot;sex&quot;&gt;Sex&lt;/h1&gt;

&lt;p&gt;In the second section of his book, Levy sets forth to argue that &lt;em&gt;sex&lt;/em&gt;
with widespread robots is inevitable. I think that Levy does a better
job of making a convincing argument in this section, in part because
he doesn’t 
have to rely on rampant futurism: no sophisticated AI capabilities are
necessary for the greater part of a sex robot’s necessary functioning,
and what is more, &lt;strong&gt;sex robots are already here&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;As Levy describes, at the time of the book’s writing hundreds of
highly realistic humanoid sex robots were being sold each year from a
single company. That company has now sold &lt;a href=&quot;http://www.nytimes.com/2015/06/12/technology/robotica-sex-robot-realdoll.html?emc=edit_th_20150612&amp;amp;nl=todaysheadlines&amp;amp;nlid=1638&amp;amp;_r=1&quot;&gt;over 5000 sex robots&lt;/a&gt;.
This highlights the importance of having discussions about this issue:
unlike super-intelligent AI, which certain famous intellectuals have
gotten worked up about, the concern with sex robots isn’t whether
they’ll exist in the next half-millenium. Sex robots are already
here, and they have the capacity to drastically alter society in the
course of the next few &lt;em&gt;decades&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Building on his description of the state of sex robots, Levy describes
a host of convincing situations in which sex robots might be used. 
I won’t directly comment on them, as each could fill its own blog
post, but I’ll list out some of them as food for thought.&lt;/p&gt;

&lt;p&gt;Sex robots may be used by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Those with STDs such as AIDS that can make it &lt;em&gt;irresponsible&lt;/em&gt; to
have sex with others.&lt;/li&gt;
  &lt;li&gt;Those for whom it’s &lt;em&gt;physically difficult&lt;/em&gt; to
have sex with others, such as the disabled.&lt;/li&gt;
  &lt;li&gt;Those for whom it’s &lt;em&gt;mentally&lt;/em&gt; difficult to have sex with others,
such as the very shy or (at least in their own minds) very ugly.&lt;/li&gt;
  &lt;li&gt;Those whose sexual desires would be physically or mentally
damaging to human partners.&lt;/li&gt;
  &lt;li&gt;Those simply wanting to have different or more interesting sex, or
those seeking to learn or practice their sexual skills.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some of these are reasons why people choose to visit
prostitutes. While there’s some concern about robots taking the jobs
of sex workers, there’s something to be said about robot sex workers
who can’t be exploited, can’t get or spread diseases, and can’t be
hurt. Before I move on, I’d like to quote some facts laid out by Levy
about the history of prostitution which I found pretty surprising.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the 1800s, it’s estimated that between 10 and 25 percent of &lt;em&gt;all
young women&lt;/em&gt; in New York City were prostitutes, making prostitution
the second largest business of the time after tailoring.&lt;/li&gt;
  &lt;li&gt;In the mid-20th century, Alfred Kinsey estimated that 69% of white
American men had been to a prostitute at least once.&lt;/li&gt;
  &lt;li&gt;A survey in the mid-20th century found that 47% of French men who
were practicing Catholics had lost their virginity to a prostitute.&lt;/li&gt;
  &lt;li&gt;In 2000, there were an estimated 30,000 male prostitutes in Thailand.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s important to note that the concerns over sex robots extend beyond
robots designed for sex. Recently, the robotics company Aldebaran
released a new robot, Pepper, shown below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;../images/laswr/pepper.jpg&quot; alt=&quot;Pepper&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;There’s nothing inherently
sexual about Pepper. At the most, you could point out that there’s a
touchscreen on Pepper’s “chest” area. But even that is a bit of a
stretch. Even so, Aldebaran felt the need to release a disclaimer that
Pepper must not be used “for sexual activity and actions for the
purpose of indecent acts, or acts for the purpose of meeting and
dating and making acquaintance of the opposite sex”. If Aldebaran
thought it was such a concern for a robot like Pepper, this perhaps
signals that this issue should engender even greater concern for
roboticists who design more human-like robots, to say nothing of &lt;a href=&quot;https://www.youtube.com/watch?v=DF39Ygp53mQ&quot;&gt;those
who develop androids&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-science-fiction-of-sex-with-robots&quot;&gt;The Science-Fiction of Sex with Robots&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;(Be prepared to skip this section if you want to avoid spoilers.)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It’s hard not to feel like this is all science fiction. And indeed,
sex robots have been showing up more and more in popular culture. The
artificially intelligent agents discussed above (i.e., those seen in
&lt;em&gt;Her&lt;/em&gt; and &lt;em&gt;Black Mirror&lt;/em&gt;) are good examples, but of course the best
recent example is &lt;em&gt;Ex Machina&lt;/em&gt;’s Ava.&lt;/p&gt;

&lt;h3 id=&quot;ex-machina&quot;&gt;Ex Machina&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../images/laswr/exmachina.jpg&quot; alt=&quot;Ex Machina&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;Ex Machina&lt;/em&gt;, Caleb (also played by Domhnall Gleeson) is brought to
a secluded woodland estate, ostensibly  to help “test” a new android,
Ava (played by Alicia Vikander), who is
incredibly humanlike, highly intelligent, and clearly designed to be
physically attractive to her creator.
As the film progresses, we learn that Ava (and other robots) have been
sexually exploited by their creator, Nathan (played by Oscar Isaac). And,
eventually, we see how Ava is able to seduce and control Caleb, and
use him to help her get away from Nathan.&lt;/p&gt;

&lt;p&gt;The questions raised by this movie are questions we’ll have to deal
with as a society as robots become more commonplace: should we be
developing humanlike robots that are overtly sexual in appearance or
behavior? Should we be developing robots that have the capacity for
suffering or sorrow? Using the arguments presented in the previous
section, it would seem that if a robot is programmed to be incapable
of suffering and is not capable of having its own goals and desires,
then such a robot cannot be the subject of cruelty, coercion,
exploitation, etc. That being said, whether the actions of a human
towards a sex robot say something about or affect &lt;em&gt;their character&lt;/em&gt; is
another question, and a valid argument against using robots for
certain purposes or treating them in a certain way. However, that’s a
topic for another day.&lt;/p&gt;

&lt;h2 id=&quot;the-science-of-sex-with-robots&quot;&gt;The Science of Sex with Robots&lt;/h2&gt;

&lt;p&gt;Obviously this topic raises a number of complicated ethical questions; and
I think that these questions are important for us to discuss as a
society. However, there has been little research thus far into 
peoples’ views on the topic. Recently, I helped my
advisor and one of my labmates conduct
the first extensive survey into people’s views on sex robots, the
results of which were the focus of a recent paper (&lt;em&gt;Are We Ready for
Sex Robots?&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;In this study, we asked participants questions in three main
categories: what they believe to be the capabilities of
sex robots, what they believe to be the appropriate uses and
appearances for sex robots, and how sex with a robot differs from sex
with a person. These questions were posed to ~100 subjects ages 20-60
through an online survey on Amazon Mechanical Turk.&lt;/p&gt;

&lt;p&gt;The study found
that men and women are in close agreement on the capabilities of sex
robots: participants generally thought that sex robots could be
instructed, could move on their own, and are specifically designed
to satisfy human desire, and that sex robots could &lt;em&gt;not&lt;/em&gt; have feelings,
recognize human emotions, or take initiative. Participants were more
evenly split on a number of other criteria, such as whether sex robots 
can adapt to human behavior, understand language, learn new
behaviors, or recognize objects. Men and women also agreed on the
differences between sex with a robot vs. sex with a person;
participants tended to say that sex with a robot would not cause one
to lose their virginity, was more like masturbation than sex with a
human, and was more like using a vibrator than having sex with a human.&lt;/p&gt;

&lt;p&gt;Men and women significantly differed, however, on the appropriate uses
and appearances for sex robots, with men systematically viewing uses
and appearances as more appropriate, typically by about one-point on a
seven-point scale. The uses for sex robots ranked most appropriate
were using them instead of prostitutes, for disabled people, to reduce
the risk of spreading STDs, to demonstrate forms of sexual
harassment (for training and prevention), and for use in isolated
environments; the uses ranked least 
appropriate were using sex robots to practice abstinence, and for sex
offenders; for all of these, the mean rating leant towards appropriate for
men and towards inappropriate for women. The appearances of sex robots
ranked most appropriate were an adult human, a fantasy creature, and
“any recognizable life form”. The forms ranked least appropriate were
an animal, one’s family member, and the only form for which ratings
did not significantly differ between men and women: a human child.
Finally, two-thirds of men said they would use a sex-robot, while
two-thirds of women said they would not.
You’ll note that the above results are all gender-based. One would
think that one would see age effects as well; but in fact, only one
significant age effect was found: Millenials rated it less appropriate
to use robots &lt;em&gt;instead of prostitutes&lt;/em&gt; than did older participants.&lt;/p&gt;

&lt;p&gt;These results give a brief look into society’s views on sex
robots. The Human-Robot Interaction community will need to further
investigate these questions through empirical study, so that we can
better understand peoples’ ethical values and principles involving sex
robots. These values will be crucial for society to make explicit as
sex robots become increasingly commonplace, and a subject of concern
for policymakers, designers, industry, and society at large.&lt;/p&gt;

&lt;h1 id=&quot;overall-recommendation&quot;&gt;Overall Recommendation&lt;/h1&gt;
&lt;p&gt;While its occasional deviations into futurism make it less than it
could have been, &lt;em&gt;Love and Sex with Robots&lt;/em&gt; is an engaging, eye
opening, and entertaining read. I recommend it highly to anyone
interested in human-robot interaction, sociology, and sexuality.&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="hri" /><category term="ethics" /><summary>In this blog post, I&#39;ll provide some of my thoughts on David Levy&#39;s &quot;Love and Sex with Robots&quot;, and provide some commentary from recent science and science fiction on the issues Levy raises (or fails to raise).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://williamstome.github.io/{&quot;feature&quot;=&gt;&quot;laswr/laswr-feature.jpg&quot;, &quot;credit&quot;=&gt;&quot;ectysis&quot;, &quot;creditlink&quot;=&gt;&quot;ectysis.wordpress.com&quot;}" /></entry><entry><title>Five Reasons Why I’m Excited for HRI 2016</title><link href="http://williamstome.github.io//off-to-hri-2016/" rel="alternate" type="text/html" title="Five Reasons Why I&#39;m Excited for HRI 2016" /><published>2016-03-02T00:00:00+00:00</published><updated>2016-03-02T00:00:00+00:00</updated><id>http://williamstome.github.io//off-to-hri-2016</id><content type="html" xml:base="http://williamstome.github.io//off-to-hri-2016/">&lt;p&gt;In just a few hours, I’ll be taking off for Christchurch, New Zealand, where I’ll be attending the 2016 International Conference on Human-Robot Interaction (HRI).
I’m especially excited for this year’s conference, for several reasons.&lt;/p&gt;

&lt;h1 id=&quot;reason-1-the-presentations&quot;&gt;Reason 1: The Presentations&lt;/h1&gt;

&lt;p&gt;As an HRI researcher, I’m obviously excited about &lt;a href=&quot;http://humanrobotinteraction.org/2016/program/&quot;&gt;the program this year&lt;/a&gt;. I’m particularly excited for the sessions on “Morality and Trust in HRI”, “Human-Robot Collaboration”, and of course, “Tools &amp;amp; Techniques for Social Robots”, in which I’m presenting &lt;a href=&quot;http://hrilab.tufts.edu/~twilliam/pubs/?p=williams2016hri&quot;&gt;a paper on human-robot natural-language communication&lt;/a&gt;. HRI is a single track conference, which means that everyone attends the same talks. This means I get to share my current research with the entire HRI community at once, which is both exciting and anxiety-inducing (but mostly exciting)!&lt;/p&gt;

&lt;h1 id=&quot;reason-2-hri-pioneers&quot;&gt;Reason 2: HRI Pioneers&lt;/h1&gt;

&lt;p&gt;Last year, I was lucky enough to be selected for the HRI Pioneers Workshop; a student-run workshop which is the “premiere forum for graduate students in HRI”. After the workshop last year, Hee-Tae Jung (from UMass Amherst) and I were elected Program Committee Co-Chairs for this year’s workshop, which means we were in charge of finding paper reviewers, assigning reviewers to papers, lightly prodding reviewers to give us their reviews, and deciding which papers would be accepted to the workshop. This was a lot of fun, and a lot of work. This year, I’m excited to pop into the workshop and learn more about the students whose abstracts we accepted. Here’s a word cloud of the most common (interesting) words found in the submissions of this years’ attendees.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/hrip2016-tagcloud.png&quot; alt=&quot;Word Cloud&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;reason-3-lab-trip&quot;&gt;Reason 3: Lab Trip!&lt;/h1&gt;

&lt;p&gt;In the past, there have usually been only one or two other people from my lab at the conferences I’ve attended. This time, there will be five of us! Three of my labmates have papers in workshops at HRI (including Pioneers!), and of course my advisor is attending. Three of us are going to have a 10-hour layover in Sydney, Australia, so I’m looking forward (I think) to several hours of jetlagged exploration of Sydney.&lt;/p&gt;

&lt;h1 id=&quot;reason-4-pushing-the-envelope&quot;&gt;Reason 4: Pushing the Envelope&lt;/h1&gt;

&lt;p&gt;This year’s program includes a new session, “Alt.HRI” which solicited papers that push boundaries and take risks. I’m interested to see just how risk-taking the papers in this new session are. On a similar note, our lab has a paper entitled “Are We Ready for Sex Robots?” in a non-Alt.HRI session which is both risky and risqué. While I implemented the study behind the paper (a survey; no sex robots in our lab!), I haven’t actually seen the results, which my advisor has been playing close to the chest… so I’m excited to hear what they are! In preparation, I’ve been reading David Levy’s &lt;a href=&quot;http://www.amazon.com/Love-Sex-Robots-Human-Robot-Relationships/dp/0061359807&quot;&gt;“Love and Sex with Robots”&lt;/a&gt;, which I highly reccomend.&lt;/p&gt;

&lt;h1 id=&quot;reason-5-its-new-zealand&quot;&gt;Reason 5: It’s New Zealand&lt;/h1&gt;
&lt;p&gt;It’s New Zealand! Enough said.&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="hri" /><category term="travel" /><summary>I&#39;m headed to New Zealand!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://williamstome.github.io/{&quot;feature&quot;=&gt;&quot;features/HRI-2016-banner-logo21.png&quot;, &quot;credit&quot;=&gt;&quot;http://humanrobotinteraction.org/&quot;}" /></entry><entry><title>(Re)designing an Introductory AI Course: Post-Semester Analysis</title><link href="http://williamstome.github.io//re-designing-an-introductory-ai-course-analysis/" rel="alternate" type="text/html" title="(Re)designing an Introductory AI Course: Post-Semester Analysis" /><published>2016-02-05T00:00:00+00:00</published><updated>2016-02-05T00:00:00+00:00</updated><id>http://williamstome.github.io//re-designing-an-introductory-ai-course-analysis</id><content type="html" xml:base="http://williamstome.github.io//re-designing-an-introductory-ai-course-analysis/">&lt;p&gt;In the Fall of 2015, I was given the opportunity to co-teach Tufts’
Artificial Intelligence class, through Tufts GIFT (Graduate 
Institute for Teaching) program. As I described in
&lt;a href=&quot;http://williamstome.github.io//initial-thoughts-on-re-designing-an-introductory-ai-course/&quot;&gt;this pre-class blog post&lt;/a&gt;,
I took the opportunity to re-design the 
class in order to include more topics from “Modern” (i.e.,
probabilistic AI). As described in that blog post, I had grand plans,
but had no idea to what extent I’d be able to implement them. 
Let’s look at how Tom’s grand plan worked out!&lt;/p&gt;

&lt;table rules=&quot;groups&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Supertopic&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Topic&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Subtopic&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Old&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Planned&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Actual&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foundations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intro&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Introduction&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1-2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1-2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1-2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foundations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Uninformed Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foundations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Heuristic Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foundations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Local Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foundations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ND Actions&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foundations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Partial Observations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foundations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CSPs&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CSPs&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foundations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CSPs&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Constraint Prop&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foundations&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Adversarial Search&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Classical AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;KR&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FOL&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;11-15,17&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Classical AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;KR&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FOL&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Classical AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Inf&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FOL Inf&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Classical AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Inf&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;FOL Inf&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;19&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Classical AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;KR&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Ontologies&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;24&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Classical AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DM&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Classical Planning&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;20-22&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Classical AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DM&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hierarchical Planning&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Classical AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DM&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Other Planning Topics&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;20,22&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;KR&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Back to Bayesics&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;25&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;KR&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Bayes Nets&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;26&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;19&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Inf&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Exact Inference&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;18-19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Inf&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Approx. Inference&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;21&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Inf&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;MCs and HMMs&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;22&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(21)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;KR&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Markov (Logic) Nets&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DM&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Utility Theory&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;24&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DM&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;MDPs&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;25&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DM&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;POMDPs&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;26&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Modern AI&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DM&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Factored MDPs&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;26&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;outcome&quot;&gt;Outcome&lt;/h2&gt;

&lt;p&gt;Two things should immediately stand out by comparing the last three
columns of the chart above.&lt;/p&gt;

&lt;p&gt;First, I was successful in convincing my co-instructor, Prof. Anselm
Blumer, to shift most of the sessions on Logic to topics from Modern
AI. This was a big surprise for me, as I initially had no idea whether
or not he’d be receptive to such a major shift. We ended up dropping
HMMs and POMDPs and picking up Factored MDPs, but otherwise the topics
covered were just what I’d hoped for.&lt;/p&gt;

&lt;p&gt;Second, though, you’ll notice that the actual class schedule jumped
around &lt;strong&gt;a lot&lt;/strong&gt;. For example, the last section of the course (16-26)
went: &lt;strong&gt;Planning&lt;/strong&gt;, Bayes Nets, Bayesian Inference, Bayesian Inference,
&lt;strong&gt;Planning&lt;/strong&gt;, Bayesian Inference, &lt;strong&gt;Planning&lt;/strong&gt;, Markov Logic,
&lt;strong&gt;Planning&lt;/strong&gt;, MDPs, MDPs.&lt;/p&gt;

&lt;p&gt;This was the product of my &lt;strong&gt;planned&lt;/strong&gt; schedule (ba-dum-psh) rubbing
up against my &lt;strong&gt;actual&lt;/strong&gt; schedule. Before the semester started, Anselm
and I worked out a rough schedule for the course, and which classes
I’d be teaching (I specifically wanted to teach CSPs, Bayes Nets, and Approximate
Inference (because I had specific in class exercises in mind), and
Markov Logic and Factored MDPs (because these are fairly new topics I
wanted to season the main course with (more puns! (nested
parentheses!)))).
However, once the semester actually started, several wrenches were
quickly thrown into the mix.&lt;/p&gt;

&lt;h2 id=&quot;best-laid-plans&quot;&gt;Best Laid Plans&lt;/h2&gt;

&lt;p&gt;First, I got papers accepted to several
conferences. While this was a good thing for my research progress, it
meant that I would no longer be at Tufts at the time I was supposed to
teach two of my sessions. To avoid changing what sessions I’d be
teaching, we had to get creative with the schedule. This involved many
small schedule shuffles, such as
putting &lt;em&gt;searching with non-deterministic actions&lt;/em&gt; off for &lt;em&gt;a month&lt;/em&gt;
in order to shift things back a week.&lt;/p&gt;

&lt;p&gt;Second, to get students ready for the modern AI section, we decided to
combine the lectures on Bayes’ Theorem and Utility Theory, and move
them earlier into the course as a sort of “preview for things to come”.&lt;/p&gt;

&lt;p&gt;Finally, of course, things just didn’t time out exactly as we
expected. Some topics took more time than expected, some took less. In
order to stay on the safe side, advanced planning topics got put off
for a bit, and then, as discussed, ended up getting peppered in
between my other lectures, whose dates were set in stone to accommodate
my conference travels. It was a whirlwind.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions…&lt;/h2&gt;

&lt;p&gt;My first time teaching was certainly a learning experience. If it
weren’t for the fact that I was scheduled to teach very specific
sessions of the class, the schedule would certainly have been saner;
presumably it will be &lt;em&gt;much&lt;/em&gt; saner when I end up teaching the course
by myself at some other university, on my schedule alone.&lt;/p&gt;

&lt;p&gt;Schedule notwithstanding, however, the semester was incredibly
rewarding. I had an absolute blast, and my students seemed to enjoy it
too. I ended up getting an average rating of 4.63 out of 5.0 on my teaching
evaluations, whatever that means, and one student wrote the following,
which made me pretty proud:&lt;/p&gt;

&lt;p&gt;“I have never
had someone put such a visible effort into each lecture and I really
appreciated seeing all the time and hard work Tom put into making sure
we had the best learning experience possible.”&lt;/p&gt;

&lt;p&gt;Of course, my students were &lt;em&gt;significantly&lt;/em&gt; less positive about some
of the homework assignments I designed. But that’s a story for another
blog post…&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="ai" /><category term="teaching" /><category term="pedagogy" /><summary>IN WHICH I compare my planned AI course to reality.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://williamstome.github.io/{&quot;feature&quot;=&gt;&quot;features/AIMA.png&quot;, &quot;credit&quot;=&gt;&quot;Artificial Intelligence- A Modern Approach&quot;}" /></entry><entry><title>The Five Stages of Grief: An HRI Lab Christmas Carol</title><link href="http://williamstome.github.io//the-five-stages-of-grief-an-hri-lab-christmas-carol/" rel="alternate" type="text/html" title="The Five Stages of Grief: An HRI Lab Christmas Carol" /><published>2015-12-18T00:00:00+00:00</published><updated>2015-12-18T00:00:00+00:00</updated><id>http://williamstome.github.io//the-five-stages-of-grief-an-hri-lab-christmas-carol</id><content type="html" xml:base="http://williamstome.github.io//the-five-stages-of-grief-an-hri-lab-christmas-carol/">&lt;p&gt;&lt;strong&gt;Warning: Less than Serious Material Ahead&lt;/strong&gt;&lt;br /&gt;
&lt;br /&gt;
Happy Holidays!&lt;br /&gt;
&lt;br /&gt;
This year has marked large changes in the HRI Lab: this semester, our
lab lost one member (to graduation! good for him, sad for us) and
gained &lt;strong&gt;eight&lt;/strong&gt;. Six new grad students started this semester, one
more just joined and will be starting next month, and we acquired a new
staff programmer.&lt;br /&gt;
&lt;br /&gt;
Anyone who knows me well knows that I love to sing. It should thus be
no surprise that I was thrilled to learn that two of my new labmates
sing! I was even able to recruit one of them to join my a cappella
group, &lt;a href=&quot;https://www.facebook.com/OutofRangeACappella/&quot;&gt;Out of Range&lt;/a&gt;.
&lt;br /&gt;
For our lab holiday party, my advisor decided that the lab band
(BA$H!, comprised of my advisor on bass and keyboard, our lab manager
on drums, our postdoc on guitar, and myself on vocals) would perform a
few songs. So, I figured, as long as I was singing, there might as
well be some a cappella action thrown in too!&lt;br /&gt;
&lt;br /&gt;
I thus put together an a cappella arrangement of holiday carols for my
two new singing labmates and I,
themed around an obviously fictitious scenario in which our lab has to
pull together a demo for a funding agency and things go horribly
wrong.&lt;br /&gt;
&lt;br /&gt;
Enjoy!&lt;/p&gt;

&lt;h2 id=&quot;the-five-stages-of-grief-an-hri-lab-christmas-carol&quot;&gt;The Five Stages of Grief: An HRI Lab Christmas Carol&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Scene: It’s Sunday night, and the grad students are busy at work in
the lab. A demo of the HRI Lab’s new “Carol Singing Robot” needs to be
given in the morning, but the robots have mysteriously stopped
working&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;pm---denial&quot;&gt;8:00 PM - Denial&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Tune: Silent Night&lt;/em&gt;&lt;br /&gt;
&lt;br /&gt;
Silent Nao, Soundless Nao&lt;br /&gt;
Working any second now&lt;br /&gt;
Nothing’s changed here, no code’s been revised&lt;br /&gt;
Why’d you stop when Matthias arrived&lt;br /&gt;
Nothing’s changed in your code&lt;br /&gt;
Nothing’s changed in your code&lt;/p&gt;

&lt;h3 id=&quot;pm---anger&quot;&gt;10:00 PM - Anger&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Tune: Santa Claus is Coming to Town&lt;/em&gt;&lt;br /&gt;
&lt;br /&gt;
You better not crash&lt;br /&gt;
You better not die&lt;br /&gt;
You better not break&lt;br /&gt;
I’m telling you why&lt;br /&gt;
&lt;br /&gt;
ONR is coming to lab&lt;br /&gt;
&lt;br /&gt;
They’ve gotta be wowed&lt;br /&gt;
They’ve gotta be stunned&lt;br /&gt;
They’re gonna decide&lt;br /&gt;
Whose project to fund&lt;br /&gt;
&lt;br /&gt;
ONR is coming to lab&lt;br /&gt;
&lt;br /&gt;
These processes keep sleeping&lt;br /&gt;
There must be some mistake&lt;br /&gt;
You can’t find java library?!&lt;br /&gt;
JUST COMPILE FOR GOODNESS SAKE!&lt;br /&gt;
&lt;br /&gt;
You better not crash&lt;br /&gt;
You better not die&lt;br /&gt;
You better not break&lt;br /&gt;
I’m telling you why&lt;br /&gt;
&lt;br /&gt;
ONR is coming to lab&lt;/p&gt;

&lt;h3 id=&quot;am---bargaining&quot;&gt;12:00 AM - Bargaining&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Tune: It Came Upon a Midnight Clear&lt;/em&gt;&lt;br /&gt;
&lt;br /&gt;
I need to finish, it’s midnight here&lt;br /&gt;
Please help me fix all of these bugs (damn bugs!)&lt;br /&gt;
I’ll buy a case of your fav’rite beer&lt;br /&gt;
I’ll give you forty-six bucks (big bucks!)&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;
Peace on the earth, a Mercedes-Benz&lt;br /&gt;
I’ll promise you ‘bout anything (more beer!)&lt;br /&gt;
If you would just help me figure out&lt;br /&gt;
How I can make this Nao sing&lt;/p&gt;

&lt;h3 id=&quot;am---depression&quot;&gt;2:00 AM - Depression&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Tune: Oh Little Town of Bethlehem&lt;/em&gt;&lt;br /&gt;
&lt;br /&gt;
So little time to get this done&lt;br /&gt;
My brain is getting fried&lt;br /&gt;
I need to leave and go to sleep&lt;br /&gt;
This isn’t justified&lt;br /&gt;
&lt;br /&gt;
Yet in the dark I’m working&lt;br /&gt;
Until I get this right&lt;br /&gt;
The jokes and jeers of all my peers&lt;br /&gt;
are met in me tonight&lt;/p&gt;

&lt;h3 id=&quot;am---acceptance&quot;&gt;4:00 AM - Acceptance&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Tune: White Christmas&lt;/em&gt;&lt;br /&gt;
&lt;br /&gt;
I’m dreaming of a live demo&lt;br /&gt;
Just like the ones I used to watch&lt;br /&gt;
No components failing&lt;br /&gt;
No students swearing&lt;br /&gt;
No robots driving into walls (Great Big Holes!)&lt;br /&gt;
&lt;br /&gt;
Oh and I’m dreaming of a live demo&lt;br /&gt;
With ev’ry action script I write&lt;br /&gt;
But until my tests get green lights&lt;br /&gt;
Let’s not do our demo live tonight&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="hri" /><category term="lab culture" /><summary>**Warning: Less than Serious Material Ahead** -- Happy Holidays!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://williamstome.github.io/{&quot;feature&quot;=&gt;&quot;features/r2d2.jpg&quot;, &quot;credit&quot;=&gt;&quot;Don Solo&quot;, &quot;creditlink&quot;=&gt;&quot;https://www.flickr.com/photos/donsolo/4211997025/&quot;}" /></entry><entry><title>Is Robot Telepathy Acceptable?</title><link href="http://williamstome.github.io//is-robot-telepathy-acceptable/" rel="alternate" type="text/html" title="Is Robot Telepathy Acceptable?" /><published>2015-10-06T00:00:00+00:00</published><updated>2015-10-06T00:00:00+00:00</updated><id>http://williamstome.github.io//is-robot-telepathy-acceptable</id><content type="html" xml:base="http://williamstome.github.io//is-robot-telepathy-acceptable/">&lt;p&gt;Imagine a future in which your house is full of robots to help you with daily tasks; cleaning, yardwork, cooking, etcetera. You come home after a long day, and plop down on the couch. Normally when this happens, your cooking and bartender robots (Waiterbot and Bottender) stop by to ask if they can get you anything. Tonight, only your cooking robot stops in.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;iframe src=&quot;//giphy.com/embed/izNuMW9MdlPwc&quot; width=&quot;480&quot; height=&quot;270&quot; frameborder=&quot;5&quot; class=&quot;giphy-embed&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;http://giphy.com/gifs/cheezburger-win-beer-izNuMW9MdlPwc&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;“Where’s Bottender?” you ask.&lt;/p&gt;

&lt;p&gt;“Bottender is plugged in in the kitchen, but should be finished charging soon,” says Waiterbot.&lt;/p&gt;

&lt;p&gt;“Can you tell Bottender I want an Old Rasputin Imperial Stout?” you ask, being far too lazy to go down to your beer cellar and get one yourself.&lt;/p&gt;

&lt;p&gt;“Certainly,” says Waiterbot, “Would you like anything to eat?”&lt;/p&gt;

&lt;p&gt;“Sure”, you say, “How about some Pad Thai.”&lt;/p&gt;

&lt;p&gt;Waiterbot agrees, and drives out of the room. As it’s leaving the room, Bottender drives in through another doorway with a bottle of Old Rasputin.&lt;br /&gt;
You gratefully take the glass from Bottender, then raise an eyebrow and ask,&lt;/p&gt;

&lt;p&gt;“How did you know I wanted an Old Rasputin?”&lt;/p&gt;

&lt;p&gt;“Waiterbot told me,” replies Bottender.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;One of the goals of the field of Human-Robot Interaction is to make human-robot interaction as &lt;em&gt;natural&lt;/em&gt; as possible. In part, this means making humans feel comfortable, and not, say, creeped out. For example,  the theory of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Uncanny_valley&quot;&gt;“Uncanny Valley”&lt;/a&gt;, which suggests that humans are creeped out by robots (or other agents) that look very close to human but are slightly “off” provides us with design suggestions for building less creepy robots.&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;http://i.imgur.com/nz9sGMk.gif&quot; alt=&quot;Uncanny Valley&quot; /&gt;&lt;br /&gt;&lt;br /&gt;
One of my tertiary research interests involves whether there is an “uncanny valley of actions”, that is, are there certain robot &lt;em&gt;behaviors&lt;/em&gt; that may be useful to the robot, but should be avoided because they creep people out. One possible action in the uncanny valley of actions is robot telepathy. As seen in the situation above, robots don’t need to communicate with each other verbally, and instead can simply communicate over WiFi.&lt;/p&gt;

&lt;p&gt;On the one hand, there’s nothing particularly insidious about this. For one thing, it’s pretty similar to humans just texting each other. Furthermore, it’s obviously much more efficient than driving up to each other and communicating through speech. On the other hand, telepathy is regarded as a “supernatural” ability which would be eerie if performed by humans, and thus it might not be unreasonable to expect that in certain situations it might seem eerie when performed by robots.&lt;/p&gt;

&lt;iframe src=&quot;//giphy.com/embed/tpYWbMVkPauEU&quot; width=&quot;480&quot; height=&quot;279&quot; frameborder=&quot;5&quot; class=&quot;giphy-embed&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;http://giphy.com/gifs/bill-murray-movie-ghostbusters-tpYWbMVkPauEU&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In a set of experiments, we decided to investigate whether people would indeed be creeped out by “robot telepathy”. We put people in a situation very much like the scenario described above, except that participants were giving the robots instructions for how to explore a disaster zone. Participants were told to assign different tasks to the two robots (looking for wounded people and looking for radiation), and to give the robots different routes to explore the environment. However, like in the scenario described above, only one robot greeted them, and told them that it would be able to pass their instructions on to the other robot, which was still charging. After giving instructions to the robots, participants entered our simulated disaster zone (a big white room filled with obstacles, with a designated “safe zone” they were told to remain in).&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;../images/telepathy/Room.png&quot; alt=&quot;Experiment Setup&quot; /&gt;&lt;br /&gt;&lt;br /&gt;
In one condition, when participants entered the room, they observed the two robots drive up to each other, and heard one robot verbally relay their instructions to the other robot. In the second condition, when participants entered the room, the robot that had been charging was already carrying out the task that the participants had instructed the other robot to relay to it.
After the experiment, we gave participants surveys in which they were asked to assess the robots on a wide variety of scales, including how creepy or unsettling they believed the robots’ behavior to be.&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;../images/telepathy/experiment_big.png&quot; alt=&quot;Experiment Setup&quot; /&gt;&lt;br /&gt;&lt;br /&gt;
What were our results? Well, as I said before, we actually ran a “set” of experiments. These experiments only differed with regards to what kind of robot interacted with them originally, and what kind of robot played the role of the “charging robot”; the second experiment merely flipped these roles. 
After the first experiment, we did not find a significant difference in perceived creepiness between the two conditions, as reported in our &lt;a href=&quot;http://hrilab.tufts.edu/~twilliam/pubs/?p=williams2014telepathy&quot;&gt;2014 RO-MAN paper&lt;/a&gt;. However, when analyzing the combined data from both experiments, we found that people rated the robots to be significantly more creepy or unsettling in the “covert” (i.e., telepathic) condition than in the “verbal” condition, as reported in our forthcoming &lt;a href=&quot;http://hrilab.tufts.edu/~twilliam/pubs/?p=williams2015jhri&quot;&gt;paper in the Journal of Human-Robot Interaction&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;../images/telepathy/creepy.png&quot; alt=&quot;Creepiness&quot; /&gt;&lt;br /&gt;&lt;br /&gt;
Now, as I’ve mentioned, this finding won’t apply in all cases. As we discuss in the JHRI article, there are a wide variety of considerations which might make robot telepathy more or less acceptable.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;It might be very &lt;em&gt;annoying or inefficient&lt;/em&gt; for robots to communicate certain information verbally: information that the human doesn’t care about, is hard to communicate “naturally”, needs to be communicated at high frequency, or for which robots would need to travel more than a trivial distance in order to communicate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It might not be &lt;em&gt;secure&lt;/em&gt; for robots to communicate some information verbally. If there are other people or robots listening in, there may be security or privacy risks associated with robots simply blurting things out to each other for everyone to hear.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If a tree falls in a forest, and there’s no one there to hear it, does it make a sound? That is, if two robots need to communicate and there are no humans listening in, does it matter if they communicate verbally? Probably not.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With these issues in mind, we concluded that it may be preferable for robots to communicate information verbally (simultaneous to more efficient nonverbal communication over wifi) purely for human benefit when (1) it’s important for humans to know that the information was communicated correctly and successfully, (2) when the transmission is going to be short, infrequent, and easy to communicate, and when (3) the only non-robot agents nearby are cooperative human agents (for which there are no security or privacy risks).&lt;/p&gt;

&lt;p&gt;This was a pretty fun research topic to investigate. Hopefully in the future I’ll be able to do more research on the uncanny valley of actions… in addition to my other research of course. Someday. When I have more time.  And a grant to investigate other aspects. Right now? I could really go for a…&lt;/p&gt;

&lt;p&gt;Wow, thanks Bottender! This is just what I wanted! Hey wait a minute…&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;http://i.imgur.com/xiqbeKb.gif&quot; alt=&quot;Uncanny Valley&quot; /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="hri" /><category term="robotics" /><category term="dialog" /><category term="uncanny valley" /><summary>One of my tertiary research interests involves whether there is an &#39;uncanny valley of actions&#39;, that is, are there certain robot *behaviors* that may be useful to the robot, but should be avoided because they creep people out.</summary></entry><entry><title>Teaching Philosophy</title><link href="http://williamstome.github.io//teaching-philosophy/" rel="alternate" type="text/html" title="Teaching Philosophy" /><published>2015-07-04T00:00:00+00:00</published><updated>2015-07-04T00:00:00+00:00</updated><id>http://williamstome.github.io//teaching-philosophy</id><content type="html" xml:base="http://williamstome.github.io//teaching-philosophy/">&lt;blockquote&gt;
  &lt;p&gt;“We need to do away with the myth that computer science is about
computers. Computer science is no more about computers than astronomy
is about telescopes, biology is about microscopes or chemistry is
about beakers and test tubes. Science is not about tools, it is about
how we use them and what we find out when we do.”&lt;/p&gt;

  &lt;p&gt;– Micheal R. Fellows and Ian Parberry&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Why do students think computer science is about computers? Due
to the scarcity of K-12 computer science education, many students’
ideas about computer science are grounded only in what they’ve seen in
popular culture, leading to a wide array of misconceptions. Contrary
to popular belief, computer security is not principally concerned with
defending against hackers 
rapidly typing inscrutable symbols. Artificial intelligence is not
principally concerned with creating anthropomorphic robots who, if they
cannot love, can at least pass the Turing test.&lt;/p&gt;

&lt;p&gt;As an educator, I seek to strip away these preconceptions, and
to show students that while computer science may be more mundane
than its Hollywood depiction, it is even more wondrous and
exciting, because it teaches a new way to think.&lt;/p&gt;

&lt;p&gt;Unfortunately, many computer science classes fall victim to the mode
of thinking criticized by Fellows &amp;amp; Parberry in the quote above. These
classes proceed 
linearly through the chosen course material, presenting each algorithm
or technique as an isolated tool to be added to the student’s toolbox.&lt;/p&gt;

&lt;p&gt;In contrast, my goal is for students to be able to look at any
problem they encounter in the world like the Necker cube shown
below. Students should see how a problem can, like the cube, be viewed in
different ways through a shift in perspective, and should understand
the relationships between those perspectives.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;../images/teachingstatement/necker_cube.png&quot; alt=&quot;necker_cube&quot; style=&quot;width: 50px;&quot; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;To foster this way of thinking, I require students to reflect deeply
on course material through both expository and persuasive writing.
This reflects my belief that development of writing skills is crucial
for computer science students.
While some tout the ability to break down problems computationally as the
“new literacy,” students will need to master “traditional literacy”
regardless of whether they go on to become researchers, educators,
programmers, or elect not to continue in computer science at all.&lt;/p&gt;

&lt;p&gt;In addition to encouraging deep reflection through structured writing,
I also strive to promote student understanding by providing concrete
groundings to theoretical concepts whenever possible. In the classroom
this is effected through exercises which demystify
concepts by showing them to be straightforward and intuitive. For
example, in the Artificial 
Intelligence course I co-designed and co-taught, students 
learned about Constraint Satisfaction Problems (CSP) by designing crossword
puzzles and then analyzing the strategies they used in that
process. Students learned about approximate inference by playing a novel
take on Clue which simulated Gibbs sampling. Outside the
classroom, I try to design programming assignments which provide
concrete computational groundings without burdening students with
busywork, i.e., coding for coding’s sake. For example, after the
in-class crossword puzzle exercise, students were asked to to implement
CSP heuristics to improve the performance of an automatic crossword
creator.&lt;/p&gt;

&lt;p&gt;In designing both programming assignments and in-class exercisese, I
try to find ways to apply 
course material to domains beyond the sciences. Skills such as
computational modeling are becoming important in suprisingly diverse areas, from
literary analysis and sociology to history and
archaeology. Emphasizing the power which both computational thinking
and computational techniques can bring to these areas, a movement
known as the “Digital Humanities,” not only serves to expand the
horizons of students in STEM fields but also to hook the interest of
students of other backgrounds. Diversity in the classroom is an asset
which can be exploited to help students discover other ways to view
the Necker cubes of their academic and personal lives.&lt;/p&gt;

&lt;p&gt;Finally, central to my teaching philosophy is the belief that learning
starts in the classroom but does not end there. 
I seek to make students aware of research opportunities where
they can further investigate the material they’ve learned in class,
and encourage them to view their classmates not only as study-buddies
but as potential teammates for collaboration outside the classroom.
I would not be where I am today were it not for the four summers of
research I was lucky to take part in as an undergraduate at Hamilton
College, and during my time as a Masters and PhD student at Tufts I
had the good fortune to mentor many undergraduate research assistants,
some of whom have gone on to graduate school themselves. I find the
process of mentoring students to be exciting; undergraduate research
is not about recruiting students to serve as research tools, but
rather how we can share the world of research with our students, and
when we do, what we can discover together.&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="ai" /><category term="teaching" /><category term="pedagogy" /><summary>Why do students think computer science is about computers?</summary></entry><entry><title>Learning Objectives for an Introductory AI Course</title><link href="http://williamstome.github.io//learning-objectives-for-an-introductory-ai-course/" rel="alternate" type="text/html" title="Learning Objectives for an Introductory AI Course" /><published>2015-06-01T00:00:00+00:00</published><updated>2015-06-01T00:00:00+00:00</updated><id>http://williamstome.github.io//learning-objectives-for-an-introductory-ai-course</id><content type="html" xml:base="http://williamstome.github.io//learning-objectives-for-an-introductory-ai-course/">&lt;p&gt;As I’ve written about in previous posts, I’m going to be co-designing and co-teaching Tufts’ “Introduction to Artificial Intelligence” course this fall. When looking over the current syllabus, the first thing I noticed was the stated learning objectives for the course:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Description and Objective: This course is an introductory survey of artificial intelligence. The course will cover the history, theory, and computational methods of artificial intelligence. Basic concepts include representation of knowledge and computational methods for reasoning. One or two application areas will be studied, to be selected from expert systems, robotics, computer vision, natural language understanding, and planning.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Among other reasons, this statement is problematic in that it is purely a course description, and doesn’t actually specify learning objectives for the course.&lt;/p&gt;

&lt;p&gt;In order to improve this statement, I used a three step process: (1) identify the set of likely student needs and backgrounds, (2) identify my overall course goal given those backgrounds, and (3) craft learning objectives to achieve that goal given those needs.&lt;/p&gt;

&lt;h2 id=&quot;student-needs&quot;&gt;Student Needs&lt;/h2&gt;

&lt;p&gt;While I’m a scientist (in training), most of the students taking my class will not be traveling that route, for better or for worse. I instead predict that the class will consist of four broad categories of students: (1) computer scientists heading towards graduate school, (2) computer scientists heading towards programming jobs, (3) cognitive &amp;amp; brain scientists, and (4) others (computer scientists who won’t continue along routes 1 and 2, and non-majors). The only course prerequisites are Data Structures and Discrete Mathematics, so while I can assume all students have some experience with programming and formal logic, I can’t assume experience analyzing algorithms (in particular, with respect to computational complexity), familiarity with search algorithms, or familiarity with probability theory.&lt;/p&gt;

&lt;h2 id=&quot;course-goal&quot;&gt;Course Goal&lt;/h2&gt;

&lt;p&gt;I obviously want my students to become familiar with AI techniques. A sub-goal of this is that I want them to become familiar with modern probabilistic AI techniques not currently taught in the course. Since everyone will have programming experience I want them to get experience actually implementing these algorithms, which will be important for those in groups 1 and 2, and possibly those in group 3. If Algorithms were a prerequisite, then I would aim for students to become confident in analyzing and deriving the AI algorithms seen in the course, but that’s not the case.&lt;/p&gt;

&lt;h2 id=&quot;learning-objectives&quot;&gt;Learning Objectives&lt;/h2&gt;

&lt;p&gt;The above analyses led me to craft the following three learning objectives.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;By the end of the semester, students should be able to identify the major classical and modern AI paradigms, and explain how they relate to each other.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;By the end of the semester, students should be able to analyze the structure of a given problem such that they can choose an appropriate AI paradigm in which to frame that problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;By the end of the semester, students should be able to implement a wide variety of both classical and modern AI algorithms.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If students achieve these objectives, then they should be able to take a given problem, identify and argue for a way in which to frame that problem, and depending on the paradigm (we won’t have time to have students implement all the algorithms we cover) be able to easily write a program to solve the problem using the given paradigm.&lt;/p&gt;

&lt;p&gt;These learning objectives will undergo revision before they actually end up on the syllabus, and will of course need to be discussed and cleared by my faculty mentor first. In the meantime, I’d appreciate any feedback I can get!&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="ai" /><category term="teaching" /><category term="pedagogy" /><summary>IN WHICH I write new learning objectives for Intro to AI</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://williamstome.github.io/{&quot;feature&quot;=&gt;&quot;features/AIMA.png&quot;, &quot;credit&quot;=&gt;&quot;Artificial Intelligence- A Modern Approach&quot;}" /></entry><entry><title>Brainstorming the Ideal Class Rhythm</title><link href="http://williamstome.github.io//brainstorming-the-ideal-class-rhythm/" rel="alternate" type="text/html" title="Brainstorming the Ideal Class Rhythm" /><published>2015-05-27T00:00:00+00:00</published><updated>2015-05-27T00:00:00+00:00</updated><id>http://williamstome.github.io//brainstorming-the-ideal-class-rhythm</id><content type="html" xml:base="http://williamstome.github.io//brainstorming-the-ideal-class-rhythm/">&lt;p&gt;A major part of GIFT training has been learning various active learning strategies: think-pair-share, ambassador groups, peer instruction, etc. For the past two days we’ve spent a good bit of time talking about “flipped classrooms”. This has made me speculate about what my ideal class “rhythm” would be, i.e., the pattern of activities that would typify before-class, in-class, and after-class learning. In this post I’m going to lay out what I think my ideal class rhythm would be, and will likely blog later about the application and execution of this rhythm in planning an actual class session, probably revolving around approximate inference. While this post is meant to be fairly general, I’ve only been thinking about the rhythm for Intro to Artificial Intelligence; if I was teaching a seminar-style class or a psychology class, different decisions might be made.&lt;/p&gt;

&lt;h2 id=&quot;before-class&quot;&gt;Before Class&lt;/h2&gt;

&lt;p&gt;The “flipped classroom” necessarily depends on students learning the basics of the material by themselves before class. In class &lt;em&gt;n-1&lt;/em&gt;, I would provide students with a list of relevant chapter sections / subsections, a list of links to video lectures covering the same content, and a set of concepts they should be familiar with before class. I would then require them to post one question on Piazza with respect to a concept they don’t fully understand, and to respond to two other students’ posts offering explanations as to those other students problems. This serves two purposes. First, it ensures that students have a venue for asking about material they are having trouble understanding. Second, by making students explain things to each other, it helps them learn the material better.&lt;/p&gt;

&lt;h2 id=&quot;during-class&quot;&gt;During Class&lt;/h2&gt;

&lt;p&gt;At the beginning of class &lt;em&gt;n&lt;/em&gt;, I would have a low-stakes quiz on the basic concepts of the reading. This provides additional incentive for students to do the assigned reading, and provides me with information so that in future iterations of the class I will be able to better portion the readings and lectures. Finally, research shows that the act of taking a quiz or test is a powerful learning tool.&lt;/p&gt;

&lt;p&gt;I would then use a combination of small-group discussions, group exercises and games to try to provide a concrete (and engaging) analogue to the course material. After each exercise, I would have a peer-evaluation-style question to ensure that students have actually learned what the exercise tried to show. Any conversations that need to be side-boarded due to time constraints I would immediately post to Piazza. This would both allow students a venue to discuss the topic after class and serve as an accountability measure for me: if I (the professor) say that I’ll look something up and get back to students on it, then having the question posted to Piazza forces me to respond to it (outside of class time) and shows the students that I’m good for my word. Finally, I would go over material left unresolved from students’ Piazza questions and tell students what they’ll need to do for class &lt;em&gt;n+1&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;after-class&quot;&gt;After Class&lt;/h2&gt;

&lt;p&gt;In addition to the readings necessary for class &lt;em&gt;n+1&lt;/em&gt;, students will of course have regular homework assignments. I see these being assigned on a weekly basis (meaning no homework beyond reading between Tuesday and Thursday), and falling into four categories:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;essay questions asking students to connect material to topics from previous units,&lt;/li&gt;
  &lt;li&gt;essay questions asking students to brainstorm extensions to algorithms and heuristics in order to account for nuances of new problems,&lt;/li&gt;
  &lt;li&gt;mathematically substantial proofs and derivations, and&lt;/li&gt;
  &lt;li&gt;coding assignments requiring students to actually implement the algorithms they’ve learned about.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All four of these are types of questions which I feel substantially extend the material covered in class; they don’t just check whether students know the facts, but assess the degree to which they’re able to apply, extend and analyze the course material.&lt;/p&gt;

&lt;p&gt;One final caveat: I’m still not entirely sure how to differentiate between what should be a homework problem and what should be a test problem. I’m anticipating that test material will be a combination of homework categories 1-3 and the types of questions asked on quizzes. Time will tell!&lt;/p&gt;</content><author><name>Tom Williams</name><email>williams@cs.tufts.edu</email><uri>http://hrilab.tufts.edu/~twilliam</uri></author><category term="ai" /><category term="teaching" /><category term="pedagogy" /><summary>A major part of GIFT training has been learning various active learning strategies...</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://williamstome.github.io/{&quot;feature&quot;=&gt;&quot;features/rhythm.png&quot;, &quot;credit&quot;=&gt;&quot;Furanda&quot;, &quot;creditlink&quot;=&gt;&quot;https://www.flickr.com/photos/furanda/&quot;}" /></entry></feed>
